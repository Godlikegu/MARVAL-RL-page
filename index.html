<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MARVAL: Masked Auto-Regressive Variational Acceleration - CVPR 2026</title>
  <meta name="description" content="Project page for MARVAL, accepted to CVPR 2026." />
  <link rel="stylesheet" href="style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;600;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
</head>
<body>
  <div class="container">
    <header class="header">
      <h1 class="title">Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning</h1>
      <h2 class="conference">CVPR 2026</h2>
      
      <div class="authors">
        <span class="author">Yuxuan Gu<sup>1</sup></span>, 
        <span class="author">Weimin Bai<sup>1</sup></span>, 
        <span class="author">Yifei Wang<sup>2</sup></span>, 
        <span class="author">Weijian Luo<sup>3</sup></span>, 
        <span class="author">He Sun<sup>1</sup></span>
      </div>
      
      <div class="affiliations">
        <span class="affiliation"><sup>1</sup>Peking University</span>
        <span class="affiliation"><sup>2</sup>Rice University</span>
        <span class="affiliation"><sup>3</sup>Xiaohongshu Inc.</span>
      </div>

      <div class="links">
        <a href="https://arxiv.org/pdf/2511.15190.pdf" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        <a href="https://arxiv.org/abs/2511.15190" class="btn" target="_blank"><i class="fas fa-book"></i> arXiv</a>
        <a href="https://github.com/Godlikegu/MARVAL-RL" class="btn" target="_blank"><i class="fab fa-github"></i> Code </a>
      </div>
    </header>

    <section class="teaser">
      <div class="figure-wrapper">
        <img src="assets/teaser.png" alt="MARVAL Teaser Figure" />
      </div>
      <p class="caption">
        <strong>Figure 1.</strong> Performance and qualitative results of MARVAL-RL. Top-left: Comparison of image quality between the MAR-B model (100 diffusion steps) and the MARVAL-RL-B model (1 diffusion step). MARVAL-RL significantly surpasses the MAR-B model in semantic quality, fidelity, and clarity. Bottom-left: Comparing FID-50k (y-axis) and inference time generating one image (x-axis) against other state-of-the-art methods. The MARVAL series (red dots) demonstrates superior performance, achieving low FID scores with significantly faster inference speeds (e.g., MARVAL-H achieves a FID of 2.00, and MARVAL-B is 32.95× faster than MAR-B). Right: A collection of diverse, high-quality images generated by MARVAL-RL-B model, showcasing its strong generative capabilities at an average speed of only 0.61 seconds per image.
      </p>
    </section>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harms the generation efficiency but also hinders the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training. To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256×256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.
      </p>
    </section>

    <section class="pipeline">
      <h2>Framework Overview</h2>
      <div class="figure-wrapper">
        <img src="assets/pipeline.png" alt="MARVAL Framework Pipeline" />
      </div>
      <p class="caption">
        <strong>Figure 2.</strong> Illustration of our overall framework. (Top-left) The MAR inference process consists of an outer auto-regressive (AR) loop and an inner diffusion chain. Starting from the class embedding token <em>c</em>, MAR performs multiple AR iterations, where each iteration predicts a new subset of latent tokens through a short diffusion process. (Bottom-left) The student one-step generator <em>g<sub>θ</sub></em> and the auxiliary network are optimized alternately. In this stage, a portion of tokens is masked, and <em>g<sub>θ</sub></em> performs a single AR iteration to predict all masked tokens guided by the teacher MAR model's CFG-based predictions. (Right) The RL refinement stage further improves perceptual fidelity. Here, the distilled generator <em>g<sub>θ</sub></em> generates images through multi-step AR inference, and a reward model evaluates the outputs based on textual prompts. The reward loss then fine-tunes <em>g<sub>θ</sub></em> to better align with human perceptual preferences.
      </p>
    </section>

    <section class="method">
      <h2>Method Summary</h2>
      <p>
        MARVAL consists of two main stages: <strong>Guided Score Implicit Matching (GSIM)</strong> for distillation and <strong>MARVAL-RL</strong> for reinforcement learning refinement.
      </p>
      <h3>Stage 1: GSIM Distillation</h3>
      <p>
        MAR uses a bi-level generation pattern: an outer AR loop controls gradual unmasking, while an inner diffusion chain runs at each iteration. GSIM compresses this costly diffusion chain into a single-step generator <em>g<sub>θ</sub>(z, c)</em>, where <em>z</em> is Gaussian noise and <em>c</em> is the autoregressive context (class embeddings, unmasked tokens, mask-position embeddings). The objective minimizes the KL divergence between the student's one-step distribution and the teacher's multi-step distribution. Since KL is intractable, GSIM uses a score-based variational formulation: the KL equals an integral of Fisher divergence over time, expressed as the squared <em>ℓ</em><sub>2</sub> distance between student and teacher score functions. An auxiliary network <em>S<sub>φ</sub></em> approximates the student score; the teacher score uses CFG-guided diffusion. The student and auxiliary parameters are optimized alternately via a gradient-equivalence theorem. Pseudo-Huber distance is used for training stability.
      </p>
      <h3>Stage 2: MARVAL-RL</h3>
      <p>
        RL cannot be coupled with GSIM because distillation trains on single-step samples (low-fidelity), while inference uses multi-step AR iterations. MARVAL-RL is therefore a separate post-hoc refinement phase. The distilled <em>g<sub>θ</sub></em> is treated as a policy that generates <em>x<sub>g</sub> = G<sub>θ</sub>(z, c<sub>emb</sub>, K)</em> via <em>K</em> AR iterations. A reward model <em>R(x<sub>g</sub>, prompt<sub>c</sub>)</em> (e.g., PickScore) scores the alignment between the generated image and the textual prompt. The RL objective maximizes expected reward by backpropagating the gradient through the reward signal. This fine-tunes <em>θ</em> to produce outputs that are both consistent with the AR prior and better aligned with human perceptual preferences.
      </p>
      <div class="highlights">
        <ul>
          <li><strong>GSIM:</strong> Score-based variational distillation compressing the diffusion chain into a single AR step while preserving flexible unmasking order.</li>
          <li><strong>MARVAL-RL:</strong> Post-hoc RL refinement using reward feedback (PickScore) on multi-step AR outputs for better perceptual fidelity and preference alignment.</li>
          <li><strong>Results:</strong> MARVAL-H achieves FID 2.00 with <em>32.95×</em> speedup on MAR-B; MARVAL-RL improves CLIP and ImageReward scores across Base, Large, and Huge scales.</li>
        </ul>
      </div>
    </section>

    <section class="results">
      <h2>Results</h2>
      <div class="metrics-grid">
        <div class="metric-box">
          <h3>FID 2.00</h3>
          <p>ImageNet 256×256</p>
        </div>
        <div class="metric-box">
          <h3>30×+</h3>
          <p>Sampling Speedup</p>
        </div>
        <div class="metric-box">
          <h3>↑ ImageReward</h3>
          <p>MARVAL-RL Gains</p>
        </div>
      </div>
    </section>

    <section class="citation">
      <h2>Citation</h2>
      <pre><code>@inproceedings{gu2026marval,
  title     = {Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning},
  author    = {Gu, Yuxuan and Bai, Weimin and Wang, Yifei and Luo, Weijian and Sun, He},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026}
}</code></pre>
    </section>
  </div>

  <footer class="footer">
    <p>© <span id="year"></span> Yuxuan Gu. Hosted on GitHub Pages.</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>